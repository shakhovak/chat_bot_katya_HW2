# HW2 generative model based chat-bot

**Задание**: необходимо разработать чат-бот, используя генеративный подход. Бот должен вести диалог как определенный персонаж сериала, имитируя стиль и манеру конкретного персонажа сериала.

## Данные
В качестве основы для чат-бота я взяла скрипты к сериалу "Теория большого взрыва", которые есть на Kaggle и можно загрузить по [ссылке](https://www.kaggle.com/code/lydia70/big-bang-theory-tv-show/input).

Основной персонаж - Шелдон Купер :)

![image](https://github.com/shakhovak/chat_bot_katya_HW2/assets/89096305/b4e0e914-3070-483f-a88a-30fae33a61f7)

Данные на kaggle уже удобно разделены на реплики каждого персонажа и на отдельные сцены (ниже принт-скрин данных)

![image](https://github.com/shakhovak/chat_bot_katya/assets/89096305/100d2802-4837-40d9-95ad-c41034e184fb)

### Данные для retrieval-элемента чат-бота

Первоначальная обработка данных схожа с той, что я делала для предыдущего домашнего задания при подготовке retrieval-based чат-бота. 

Обработка данных подразумевает следующие шаги:
- отбор реплик персонажа в качестве ответов / answers. Именно из этих реплик будет выбирать бот свой ответ на высказывание пользователя.
- выделение предшествующей фразы как вопроса. Если это фраза первая в сцене, то это поле будет пустым.
- отбор предыдущих реплик как контекста диалога (ограничение не более 5 фраз в контексте). Если фраза первая в сцене, то контекст также будет пустым. Контекст - идущие подряд предложения. Я не стала разбивать на диалоги, как предлагалось на семинаре.
- сохранение файла в pickle формат для последующего использования алгоритмом (файл scripts.pkl)

Так как я буду использовать дополнение генерации retrieval данными по косинусной близости, то все начальные данные векторизую в базу данных (файл scripts_vectors.pkl). Для векторизации использую обученную в предыдущем задании модель bi-encoder, которую я сохранила в мой репозиторий на Hugging Face ([ссылка](https://huggingface.co/Shakhovak/chatbot_sentence-transformer)). Детально про обучение этой модели - в предыдущем задании ([ссылка на репозиторий](https://github.com/shakhovak/chat_bot_katya/tree/master))

Функции для обработки данных находятся в файле ```utils.py```:
1. ```scripts_rework_ranking``` - перерабатывает файл, как описано выше и сохраняет их в pkl формате
2. ```encode_df_save``` - использует переработанный файл и векторизует его, а вектора уже сохраняет в базу
   
### Данные для генеративной модели

Для обучение генеравной модели я использую первоначальную переработку как и для retrieval-данных, описанную выше. Дополнительно, я разбиваю весь полученный контекст на части: если в контексте есть три предложения, то я в итоге получаю 4 семпла для данных:

- ответ + вопрос + предложение 3 +  предложение 2 + предложение 1
- ответ + вопрос + предложение 3 +  предложение 2
- ответ + вопрос + предложение 3
- ответ + вопрос

Таким образом, получается порядка 50 тыс. семплов для обучения. Обработанные и дополненные таким образом данные сохранены в файл scripts_reworked.pkl.

Функции для обработки данных находятся в файле ```utils.py```:
1. ```scripts_rework``` - перерабатывает файл, как описано выше и сохраняет их в pkl формате

## Архитектура чат-бота

Схематично процесс работы чат-бота представлен на рисунке ниже.

![image](https://github.com/shakhovak/chat_bot_katya_HW2/assets/89096305/7fa47235-c805-439f-bdea-c4ed203c12d4)

### Retrieval-часть чат-бота

**База данных реплик** включает векторизованные при помощи модели [обученного_энкодера](https://huggingface.co/Shakhovak/chatbot_sentence-transformer) скрипты, включающие контекст и вопрос. Детальное описание процесса обучения я приводила в предыдущем задании. Здесь же просто воспользуюсь готовой моделью из моего репозитория на Hugging Face ([ссылка](https://huggingface.co/Shakhovak/chatbot_sentence-transformer)).

Реплика из базы данных, вопрос и контекст который максимально похожи на тещий запрос в обработке чат-бота, будет подаваться на вход генеративной языковой модели как часть контекста при реализации стратегии RAG (retrieval-augmented generation) для того, чтобы немного добавить фактов и деталей из сценария сериала в диалого с пользователем.

### Generative-часть чат-бота
Основной частью чат-бота является генеративная модель. Так как мне удалось получить довольно значительное количество исходных данных, то можно попробовать дообучить небольшие модели семейства T5, которые известны своей универсальностью. В качестве основной для своего эксперимента я выбрала модель ```google/flan-t5-base``` на 248 млн. параметров (детально про модель можно посмотреть [здесь](https://huggingface.co/google/flan-t5-base) в репозитории Hugging Face). Обучаю я модель в облаке в ВМ c 



