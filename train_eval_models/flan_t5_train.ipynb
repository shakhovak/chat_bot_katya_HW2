{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:25.264456Z",
     "iopub.status.busy": "2024-03-07T10:05:25.264094Z",
     "iopub.status.idle": "2024-03-07T10:05:25.283889Z",
     "shell.execute_reply": "2024-03-07T10:05:25.283309Z",
     "shell.execute_reply.started": "2024-03-07T10:05:25.264436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:25.368259Z",
     "iopub.status.busy": "2024-03-07T10:05:25.367858Z",
     "iopub.status.idle": "2024-03-07T10:05:25.376747Z",
     "shell.execute_reply": "2024-03-07T10:05:25.376208Z",
     "shell.execute_reply.started": "2024-03-07T10:05:25.368240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install -q -U einops datasets matplotlib tqdm boto3 git+https://github.com/dask/s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:25.480123Z",
     "iopub.status.busy": "2024-03-07T10:05:25.479762Z",
     "iopub.status.idle": "2024-03-07T10:05:25.488332Z",
     "shell.execute_reply": "2024-03-07T10:05:25.487814Z",
     "shell.execute_reply.started": "2024-03-07T10:05:25.480105Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:25.591082Z",
     "iopub.status.busy": "2024-03-07T10:05:25.590818Z",
     "iopub.status.idle": "2024-03-07T10:05:25.598955Z",
     "shell.execute_reply": "2024-03-07T10:05:25.598415Z",
     "shell.execute_reply.started": "2024-03-07T10:05:25.591066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:25.701977Z",
     "iopub.status.busy": "2024-03-07T10:05:25.701588Z",
     "iopub.status.idle": "2024-03-07T10:05:25.711182Z",
     "shell.execute_reply": "2024-03-07T10:05:25.710642Z",
     "shell.execute_reply.started": "2024-03-07T10:05:25.701960Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:25.814198Z",
     "iopub.status.busy": "2024-03-07T10:05:25.813717Z",
     "iopub.status.idle": "2024-03-07T10:05:25.822809Z",
     "shell.execute_reply": "2024-03-07T10:05:25.822294Z",
     "shell.execute_reply.started": "2024-03-07T10:05:25.814181Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:25.926733Z",
     "iopub.status.busy": "2024-03-07T10:05:25.926234Z",
     "iopub.status.idle": "2024-03-07T10:05:25.943489Z",
     "shell.execute_reply": "2024-03-07T10:05:25.942955Z",
     "shell.execute_reply.started": "2024-03-07T10:05:25.926706Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:26.046256Z",
     "iopub.status.busy": "2024-03-07T10:05:26.046018Z",
     "iopub.status.idle": "2024-03-07T10:05:26.054644Z",
     "shell.execute_reply": "2024-03-07T10:05:26.054128Z",
     "shell.execute_reply.started": "2024-03-07T10:05:26.046240Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:26.157753Z",
     "iopub.status.busy": "2024-03-07T10:05:26.157240Z",
     "iopub.status.idle": "2024-03-07T10:05:26.167631Z",
     "shell.execute_reply": "2024-03-07T10:05:26.167094Z",
     "shell.execute_reply.started": "2024-03-07T10:05:26.157722Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from utils import scripts_rework\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:26.270857Z",
     "iopub.status.busy": "2024-03-07T10:05:26.270210Z",
     "iopub.status.idle": "2024-03-07T10:05:26.314747Z",
     "shell.execute_reply": "2024-03-07T10:05:26.314221Z",
     "shell.execute_reply.started": "2024-03-07T10:05:26.270840Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thereâ€™s no point, I just think itâ€™s a good id...</td>\n",
       "      <td>Agreed, whatâ€™s your point?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>Hang on.  One across is Aegean, eight down is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>One across is Aegean, eight down is Nabakov, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>Can I help you? Yes. Um, is this the High IQ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>Yes. Um, is this the High IQ sperm bank?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48859</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>Well, perfect. I made us sandwiches. How thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48860</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>How thoughtful. Thank you. Mmm. No big deal, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48861</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>Mmm. No big deal, I enjoy spending time with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48862</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>And I with you. Question, are you seeking a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48863</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48864 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  answer  ...                                            context\n",
       "0       Thereâ€™s no point, I just think itâ€™s a good id...  ...                                                   \n",
       "1                             I think this is the place.  ...   Hang on.  One across is Aegean, eight down is...\n",
       "2                             I think this is the place.  ...   One across is Aegean, eight down is Nabakov, ...\n",
       "3                             I think this is the place.  ...   Can I help you? Yes. Um, is this the High IQ ...\n",
       "4                             I think this is the place.  ...           Yes. Um, is this the High IQ sperm bank?\n",
       "...                                                  ...  ...                                                ...\n",
       "48859   Well, that would raise a number of problems. ...  ...   Well, perfect. I made us sandwiches. How thou...\n",
       "48860   Well, that would raise a number of problems. ...  ...   How thoughtful. Thank you. Mmm. No big deal, ...\n",
       "48861   Well, that would raise a number of problems. ...  ...   Mmm. No big deal, I enjoy spending time with ...\n",
       "48862   Well, that would raise a number of problems. ...  ...   And I with you. Question, are you seeking a r...\n",
       "48863   Well, that would raise a number of problems. ...  ...                                                   \n",
       "\n",
       "[48864 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/scripts_reworked.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:26.376989Z",
     "iopub.status.busy": "2024-03-07T10:05:26.376749Z",
     "iopub.status.idle": "2024-03-07T10:05:26.724392Z",
     "shell.execute_reply": "2024-03-07T10:05:26.723842Z",
     "shell.execute_reply.started": "2024-03-07T10:05:26.376971Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thereâ€™s no point, I just think itâ€™s a good id...</td>\n",
       "      <td>Agreed, whatâ€™s your point?</td>\n",
       "      <td></td>\n",
       "      <td>context: &lt;/s&gt;question:  Agreed, whatâ€™s your po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>Hang on.  One across is Aegean, eight down is...</td>\n",
       "      <td>context:  Hang on.  One across is Aegean, eigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>One across is Aegean, eight down is Nabakov, ...</td>\n",
       "      <td>context:  One across is Aegean, eight down is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>Can I help you? Yes. Um, is this the High IQ ...</td>\n",
       "      <td>context:  Can I help you? Yes. Um, is this the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think this is the place.</td>\n",
       "      <td>If you have to ask, maybe you shouldnâ€™t be here.</td>\n",
       "      <td>Yes. Um, is this the High IQ sperm bank?</td>\n",
       "      <td>context:  Yes. Um, is this the High IQ sperm b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48859</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>Well, perfect. I made us sandwiches. How thou...</td>\n",
       "      <td>context:  Well, perfect. I made us sandwiches....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48860</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>How thoughtful. Thank you. Mmm. No big deal, ...</td>\n",
       "      <td>context:  How thoughtful. Thank you. Mmm. No b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48861</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>Mmm. No big deal, I enjoy spending time with ...</td>\n",
       "      <td>context:  Mmm. No big deal, I enjoy spending t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48862</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td>And I with you. Question, are you seeking a r...</td>\n",
       "      <td>context:  And I with you. Question, are you se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48863</th>\n",
       "      <td>Well, that would raise a number of problems. ...</td>\n",
       "      <td>What if I were?</td>\n",
       "      <td></td>\n",
       "      <td>context: &lt;/s&gt;question:  What if I were?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48864 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  answer  ...                                           combined\n",
       "0       Thereâ€™s no point, I just think itâ€™s a good id...  ...  context: </s>question:  Agreed, whatâ€™s your po...\n",
       "1                             I think this is the place.  ...  context:  Hang on.  One across is Aegean, eigh...\n",
       "2                             I think this is the place.  ...  context:  One across is Aegean, eight down is ...\n",
       "3                             I think this is the place.  ...  context:  Can I help you? Yes. Um, is this the...\n",
       "4                             I think this is the place.  ...  context:  Yes. Um, is this the High IQ sperm b...\n",
       "...                                                  ...  ...                                                ...\n",
       "48859   Well, that would raise a number of problems. ...  ...  context:  Well, perfect. I made us sandwiches....\n",
       "48860   Well, that would raise a number of problems. ...  ...  context:  How thoughtful. Thank you. Mmm. No b...\n",
       "48861   Well, that would raise a number of problems. ...  ...  context:  Mmm. No big deal, I enjoy spending t...\n",
       "48862   Well, that would raise a number of problems. ...  ...  context:  And I with you. Question, are you se...\n",
       "48863   Well, that would raise a number of problems. ...  ...            context: </s>question:  What if I were?\n",
       "\n",
       "[48864 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"combined\"] = df[[\"question\", \"context\"]].apply(\n",
    "    lambda df: \"context: \" + df[\"context\"] + \"</s>\" + 'question: '\n",
    "+ df['question'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:26.725435Z",
     "iopub.status.busy": "2024-03-07T10:05:26.725175Z",
     "iopub.status.idle": "2024-03-07T10:05:27.012442Z",
     "shell.execute_reply": "2024-03-07T10:05:27.011950Z",
     "shell.execute_reply.started": "2024-03-07T10:05:26.725418Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48375, 4), (489, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.01, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:27.013816Z",
     "iopub.status.busy": "2024-03-07T10:05:27.013507Z",
     "iopub.status.idle": "2024-03-07T10:05:27.814426Z",
     "shell.execute_reply": "2024-03-07T10:05:27.813761Z",
     "shell.execute_reply.started": "2024-03-07T10:05:27.013797Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'question', 'context', 'combined'],\n",
       "        num_rows: 48375\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['answer', 'question', 'context', 'combined'],\n",
       "        num_rows: 489\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "        \"valid\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    }\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:27.815811Z",
     "iopub.status.busy": "2024-03-07T10:05:27.815448Z",
     "iopub.status.idle": "2024-03-07T10:05:30.616216Z",
     "shell.execute_reply": "2024-03-07T10:05:30.615614Z",
     "shell.execute_reply.started": "2024-03-07T10:05:27.815792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:30.617447Z",
     "iopub.status.busy": "2024-03-07T10:05:30.617001Z",
     "iopub.status.idle": "2024-03-07T10:05:35.265879Z",
     "shell.execute_reply": "2024-03-07T10:05:35.265021Z",
     "shell.execute_reply.started": "2024-03-07T10:05:30.617427Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7fd1cffb6b00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7a24b7001241f5ae7183f6c14b2e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e58efceb5741d0abb657c0fb5535c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 304\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"valid\"]]).map(\n",
    "    lambda x: tokenizer(x[\"combined\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"context\", \"question\", \"answer\", \"combined\"],\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"valid\"]]).map(\n",
    "    lambda x: tokenizer(x[\"answer\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"context\", \"question\", \"answer\", \"combined\"],\n",
    ")\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:35.267160Z",
     "iopub.status.busy": "2024-03-07T10:05:35.266764Z",
     "iopub.status.idle": "2024-03-07T10:05:55.929806Z",
     "shell.execute_reply": "2024-03-07T10:05:55.929259Z",
     "shell.execute_reply.started": "2024-03-07T10:05:35.267139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397481224e444f31a5bbdf9eef17432c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48375 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c941b95175de45ba81dac66f88ecc124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/489 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        sample['combined'], max_length=max_source_length, padding=padding, truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(\n",
    "        text_target=sample[\"answer\"],\n",
    "        max_length=max_target_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"context\", \"question\", \"answer\", \"combined\"]\n",
    ")\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:55.930683Z",
     "iopub.status.busy": "2024-03-07T10:05:55.930404Z",
     "iopub.status.idle": "2024-03-07T10:05:57.191204Z",
     "shell.execute_reply": "2024-03-07T10:05:57.190437Z",
     "shell.execute_reply.started": "2024-03-07T10:05:55.930666Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:05:57.193189Z",
     "iopub.status.busy": "2024-03-07T10:05:57.192865Z",
     "iopub.status.idle": "2024-03-07T10:06:03.265641Z",
     "shell.execute_reply": "2024-03-07T10:06:03.264953Z",
     "shell.execute_reply.started": "2024-03-07T10:05:57.193170Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 10:05:58.360094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c198c78844fc4e35ba415914cb12fdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c988730a63d40e699fba48f5bb421ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels,lang=\"en\")\n",
    "    \n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    rouge_result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    result = {'rouge_1': round(rouge_result['rouge1'], 4),\n",
    "              'rouge_2': round(rouge_result['rouge2'], 4),\n",
    "              'rouge_L': round(rouge_result['rougeL'], 4), \n",
    "              'avg_len': round(rouge_result['gen_len'], 4), \n",
    "              'bertscore_prec': round(np.mean(bertscore_result['precision']), 4),\n",
    "              'bertscore_rec': round(np.mean(bertscore_result['recall']), 4),\n",
    "              'bertscore_f1': round(np.mean(bertscore_result['f1']), 4)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:06:03.267012Z",
     "iopub.status.busy": "2024-03-07T10:06:03.266533Z",
     "iopub.status.idle": "2024-03-07T10:06:03.285453Z",
     "shell.execute_reply": "2024-03-07T10:06:03.284871Z",
     "shell.execute_reply.started": "2024-03-07T10:06:03.266992Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=label_pad_token_id, pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:06:03.286317Z",
     "iopub.status.busy": "2024-03-07T10:06:03.286088Z",
     "iopub.status.idle": "2024-03-07T10:06:03.297433Z",
     "shell.execute_reply": "2024-03-07T10:06:03.296845Z",
     "shell.execute_reply.started": "2024-03-07T10:06:03.286301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HfFolder.save_token(os.environ['hugging_face_login'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:06:03.298311Z",
     "iopub.status.busy": "2024-03-07T10:06:03.298072Z",
     "iopub.status.idle": "2024-03-07T10:06:05.718116Z",
     "shell.execute_reply": "2024-03-07T10:06:05.717437Z",
     "shell.execute_reply.started": "2024-03-07T10:06:03.298295Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkatya_shakhova\u001b[0m (\u001b[33mshakhova\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ[\"WANDB_PROJECT\"] = \"generative_models_chat\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "wandb.login(key=os.environ[\"wandb_login\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:06:05.719164Z",
     "iopub.status.busy": "2024-03-07T10:06:05.718889Z",
     "iopub.status.idle": "2024-03-07T10:06:05.984952Z",
     "shell.execute_reply": "2024-03-07T10:06:05.984278Z",
     "shell.execute_reply.started": "2024-03-07T10:06:05.719146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-sheldon-chat-v2\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    # logging & evaluation strategies\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=200,\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T10:06:05.985923Z",
     "iopub.status.busy": "2024-03-07T10:06:05.985685Z",
     "iopub.status.idle": "2024-03-07T13:03:30.347335Z",
     "shell.execute_reply": "2024-03-07T13:03:30.346647Z",
     "shell.execute_reply.started": "2024-03-07T10:06:05.985905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jupyter/work/resources/chat_bot_katya_HW2/wandb/run-20240307_100606-t5nq3oct\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcelestial-hill-37\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/shakhova/generative_models_chat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/shakhova/generative_models_chat/runs/t5nq3oct\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15120' max='15120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15120/15120 2:57:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge 1</th>\n",
       "      <th>Rouge 2</th>\n",
       "      <th>Rouge L</th>\n",
       "      <th>Avg Len</th>\n",
       "      <th>Bertscore Prec</th>\n",
       "      <th>Bertscore Rec</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.573100</td>\n",
       "      <td>3.255625</td>\n",
       "      <td>9.288600</td>\n",
       "      <td>1.782600</td>\n",
       "      <td>8.855800</td>\n",
       "      <td>10.881400</td>\n",
       "      <td>0.866100</td>\n",
       "      <td>0.849400</td>\n",
       "      <td>0.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.421000</td>\n",
       "      <td>3.175784</td>\n",
       "      <td>9.255900</td>\n",
       "      <td>1.906400</td>\n",
       "      <td>8.824300</td>\n",
       "      <td>10.707600</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>0.853800</td>\n",
       "      <td>0.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.345700</td>\n",
       "      <td>3.119731</td>\n",
       "      <td>9.099100</td>\n",
       "      <td>2.185400</td>\n",
       "      <td>8.707600</td>\n",
       "      <td>10.106300</td>\n",
       "      <td>0.874100</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.301300</td>\n",
       "      <td>3.075485</td>\n",
       "      <td>8.552300</td>\n",
       "      <td>1.889600</td>\n",
       "      <td>8.222400</td>\n",
       "      <td>12.368100</td>\n",
       "      <td>0.870600</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.250900</td>\n",
       "      <td>3.038346</td>\n",
       "      <td>9.239600</td>\n",
       "      <td>2.040900</td>\n",
       "      <td>8.831400</td>\n",
       "      <td>12.051100</td>\n",
       "      <td>0.872900</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.237600</td>\n",
       "      <td>3.000550</td>\n",
       "      <td>8.735600</td>\n",
       "      <td>1.818100</td>\n",
       "      <td>8.303900</td>\n",
       "      <td>12.979600</td>\n",
       "      <td>0.870200</td>\n",
       "      <td>0.854100</td>\n",
       "      <td>0.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.207500</td>\n",
       "      <td>2.960210</td>\n",
       "      <td>9.430300</td>\n",
       "      <td>2.000300</td>\n",
       "      <td>8.757100</td>\n",
       "      <td>16.073600</td>\n",
       "      <td>0.863300</td>\n",
       "      <td>0.854800</td>\n",
       "      <td>0.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.134200</td>\n",
       "      <td>2.926318</td>\n",
       "      <td>10.814800</td>\n",
       "      <td>2.269800</td>\n",
       "      <td>9.873500</td>\n",
       "      <td>15.482600</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.137600</td>\n",
       "      <td>2.891541</td>\n",
       "      <td>9.938100</td>\n",
       "      <td>1.856900</td>\n",
       "      <td>9.129100</td>\n",
       "      <td>15.171800</td>\n",
       "      <td>0.863500</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.086200</td>\n",
       "      <td>2.864813</td>\n",
       "      <td>9.653400</td>\n",
       "      <td>1.792500</td>\n",
       "      <td>8.953900</td>\n",
       "      <td>14.993900</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.853800</td>\n",
       "      <td>0.856700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.063300</td>\n",
       "      <td>2.837162</td>\n",
       "      <td>7.457300</td>\n",
       "      <td>1.688300</td>\n",
       "      <td>7.268800</td>\n",
       "      <td>15.411000</td>\n",
       "      <td>0.859700</td>\n",
       "      <td>0.851200</td>\n",
       "      <td>0.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.028600</td>\n",
       "      <td>2.811822</td>\n",
       "      <td>9.964000</td>\n",
       "      <td>2.126900</td>\n",
       "      <td>9.281000</td>\n",
       "      <td>13.570600</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>0.862800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.024800</td>\n",
       "      <td>2.783926</td>\n",
       "      <td>8.968800</td>\n",
       "      <td>1.901000</td>\n",
       "      <td>8.543900</td>\n",
       "      <td>14.566500</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.852700</td>\n",
       "      <td>0.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.977700</td>\n",
       "      <td>2.763659</td>\n",
       "      <td>10.233800</td>\n",
       "      <td>2.073700</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>13.341500</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>0.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.980300</td>\n",
       "      <td>2.743752</td>\n",
       "      <td>8.870500</td>\n",
       "      <td>1.811500</td>\n",
       "      <td>8.349100</td>\n",
       "      <td>15.791400</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.931300</td>\n",
       "      <td>2.719553</td>\n",
       "      <td>8.500100</td>\n",
       "      <td>1.839600</td>\n",
       "      <td>8.155500</td>\n",
       "      <td>14.400800</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.909400</td>\n",
       "      <td>2.701286</td>\n",
       "      <td>9.920100</td>\n",
       "      <td>1.895800</td>\n",
       "      <td>9.186300</td>\n",
       "      <td>16.077700</td>\n",
       "      <td>0.858700</td>\n",
       "      <td>0.854200</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.862500</td>\n",
       "      <td>2.680440</td>\n",
       "      <td>9.667500</td>\n",
       "      <td>2.041200</td>\n",
       "      <td>9.110700</td>\n",
       "      <td>13.523500</td>\n",
       "      <td>0.869100</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.887100</td>\n",
       "      <td>2.659496</td>\n",
       "      <td>9.985300</td>\n",
       "      <td>2.036500</td>\n",
       "      <td>9.347100</td>\n",
       "      <td>14.983600</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.860700</td>\n",
       "      <td>2.643632</td>\n",
       "      <td>9.572600</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>8.909600</td>\n",
       "      <td>14.415100</td>\n",
       "      <td>0.863500</td>\n",
       "      <td>0.854100</td>\n",
       "      <td>0.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.802800</td>\n",
       "      <td>2.629830</td>\n",
       "      <td>9.650100</td>\n",
       "      <td>1.848400</td>\n",
       "      <td>9.113100</td>\n",
       "      <td>13.476500</td>\n",
       "      <td>0.863600</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>0.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.835700</td>\n",
       "      <td>2.607998</td>\n",
       "      <td>8.961000</td>\n",
       "      <td>1.992300</td>\n",
       "      <td>8.577600</td>\n",
       "      <td>13.611500</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>0.858100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.808500</td>\n",
       "      <td>2.595366</td>\n",
       "      <td>9.294200</td>\n",
       "      <td>1.807200</td>\n",
       "      <td>8.701800</td>\n",
       "      <td>15.942700</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>0.852900</td>\n",
       "      <td>0.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.785300</td>\n",
       "      <td>2.577303</td>\n",
       "      <td>9.686500</td>\n",
       "      <td>2.001000</td>\n",
       "      <td>9.105800</td>\n",
       "      <td>14.662600</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.854800</td>\n",
       "      <td>0.859900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.793400</td>\n",
       "      <td>2.561251</td>\n",
       "      <td>9.363500</td>\n",
       "      <td>2.001400</td>\n",
       "      <td>8.871300</td>\n",
       "      <td>13.640100</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.769700</td>\n",
       "      <td>2.550842</td>\n",
       "      <td>8.631400</td>\n",
       "      <td>1.781200</td>\n",
       "      <td>8.244600</td>\n",
       "      <td>13.441700</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.852300</td>\n",
       "      <td>0.858800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.745100</td>\n",
       "      <td>2.536112</td>\n",
       "      <td>9.779900</td>\n",
       "      <td>2.152400</td>\n",
       "      <td>9.252700</td>\n",
       "      <td>12.392600</td>\n",
       "      <td>0.871100</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.769200</td>\n",
       "      <td>2.521983</td>\n",
       "      <td>9.770700</td>\n",
       "      <td>2.258000</td>\n",
       "      <td>9.204200</td>\n",
       "      <td>14.010200</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.853900</td>\n",
       "      <td>0.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.743500</td>\n",
       "      <td>2.510960</td>\n",
       "      <td>9.698200</td>\n",
       "      <td>2.174200</td>\n",
       "      <td>9.215700</td>\n",
       "      <td>14.773000</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.853600</td>\n",
       "      <td>0.857400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.703300</td>\n",
       "      <td>2.494797</td>\n",
       "      <td>9.776600</td>\n",
       "      <td>2.084300</td>\n",
       "      <td>9.141000</td>\n",
       "      <td>13.392600</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.666400</td>\n",
       "      <td>2.479158</td>\n",
       "      <td>10.401900</td>\n",
       "      <td>2.224700</td>\n",
       "      <td>9.815500</td>\n",
       "      <td>13.517400</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.854700</td>\n",
       "      <td>0.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.654200</td>\n",
       "      <td>2.470754</td>\n",
       "      <td>10.233900</td>\n",
       "      <td>2.209600</td>\n",
       "      <td>9.591400</td>\n",
       "      <td>14.766900</td>\n",
       "      <td>0.866800</td>\n",
       "      <td>0.854800</td>\n",
       "      <td>0.860300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.646700</td>\n",
       "      <td>2.458221</td>\n",
       "      <td>10.302100</td>\n",
       "      <td>2.352800</td>\n",
       "      <td>9.757800</td>\n",
       "      <td>12.944800</td>\n",
       "      <td>0.872300</td>\n",
       "      <td>0.854700</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.637600</td>\n",
       "      <td>2.447728</td>\n",
       "      <td>10.305600</td>\n",
       "      <td>2.185400</td>\n",
       "      <td>9.849900</td>\n",
       "      <td>12.948900</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.626700</td>\n",
       "      <td>2.436406</td>\n",
       "      <td>10.635800</td>\n",
       "      <td>2.243400</td>\n",
       "      <td>10.002000</td>\n",
       "      <td>13.425400</td>\n",
       "      <td>0.869100</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>0.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.648900</td>\n",
       "      <td>2.424560</td>\n",
       "      <td>10.545900</td>\n",
       "      <td>2.193500</td>\n",
       "      <td>9.853100</td>\n",
       "      <td>13.494900</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>0.854900</td>\n",
       "      <td>0.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.617400</td>\n",
       "      <td>2.415971</td>\n",
       "      <td>9.894300</td>\n",
       "      <td>2.212600</td>\n",
       "      <td>9.211100</td>\n",
       "      <td>15.016400</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.609400</td>\n",
       "      <td>2.405221</td>\n",
       "      <td>10.894100</td>\n",
       "      <td>2.324600</td>\n",
       "      <td>10.118200</td>\n",
       "      <td>14.533700</td>\n",
       "      <td>0.867600</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>0.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.587700</td>\n",
       "      <td>2.396639</td>\n",
       "      <td>10.509200</td>\n",
       "      <td>2.239800</td>\n",
       "      <td>9.779200</td>\n",
       "      <td>14.454000</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.552500</td>\n",
       "      <td>2.384467</td>\n",
       "      <td>10.087800</td>\n",
       "      <td>2.170100</td>\n",
       "      <td>9.413200</td>\n",
       "      <td>13.905900</td>\n",
       "      <td>0.867800</td>\n",
       "      <td>0.854700</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.591200</td>\n",
       "      <td>2.370852</td>\n",
       "      <td>10.504000</td>\n",
       "      <td>2.308100</td>\n",
       "      <td>9.833600</td>\n",
       "      <td>13.707600</td>\n",
       "      <td>0.868600</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>0.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.574800</td>\n",
       "      <td>2.362451</td>\n",
       "      <td>10.616900</td>\n",
       "      <td>2.421600</td>\n",
       "      <td>10.005300</td>\n",
       "      <td>13.920200</td>\n",
       "      <td>0.867100</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.592100</td>\n",
       "      <td>2.355034</td>\n",
       "      <td>10.423300</td>\n",
       "      <td>2.328300</td>\n",
       "      <td>9.930500</td>\n",
       "      <td>14.196300</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>0.854700</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.561700</td>\n",
       "      <td>2.341361</td>\n",
       "      <td>10.587700</td>\n",
       "      <td>2.349500</td>\n",
       "      <td>9.980700</td>\n",
       "      <td>13.789400</td>\n",
       "      <td>0.866900</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.563200</td>\n",
       "      <td>2.333764</td>\n",
       "      <td>10.874400</td>\n",
       "      <td>2.521200</td>\n",
       "      <td>10.228100</td>\n",
       "      <td>14.102200</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.525700</td>\n",
       "      <td>2.321663</td>\n",
       "      <td>11.321500</td>\n",
       "      <td>2.499500</td>\n",
       "      <td>10.589000</td>\n",
       "      <td>14.926400</td>\n",
       "      <td>0.864700</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>0.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.488900</td>\n",
       "      <td>2.315444</td>\n",
       "      <td>11.260800</td>\n",
       "      <td>2.410300</td>\n",
       "      <td>10.531900</td>\n",
       "      <td>14.104300</td>\n",
       "      <td>0.866800</td>\n",
       "      <td>0.855400</td>\n",
       "      <td>0.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.517300</td>\n",
       "      <td>2.307708</td>\n",
       "      <td>10.901700</td>\n",
       "      <td>2.333100</td>\n",
       "      <td>10.335200</td>\n",
       "      <td>14.431500</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.515100</td>\n",
       "      <td>2.300573</td>\n",
       "      <td>10.728600</td>\n",
       "      <td>2.259800</td>\n",
       "      <td>10.209300</td>\n",
       "      <td>13.728000</td>\n",
       "      <td>0.866600</td>\n",
       "      <td>0.855700</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.488300</td>\n",
       "      <td>2.295188</td>\n",
       "      <td>10.858000</td>\n",
       "      <td>2.373500</td>\n",
       "      <td>10.332900</td>\n",
       "      <td>14.122700</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>0.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.500600</td>\n",
       "      <td>2.286124</td>\n",
       "      <td>10.789200</td>\n",
       "      <td>2.259800</td>\n",
       "      <td>10.138200</td>\n",
       "      <td>13.425400</td>\n",
       "      <td>0.868800</td>\n",
       "      <td>0.856300</td>\n",
       "      <td>0.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.466100</td>\n",
       "      <td>2.282678</td>\n",
       "      <td>10.917000</td>\n",
       "      <td>2.389200</td>\n",
       "      <td>10.193300</td>\n",
       "      <td>13.969300</td>\n",
       "      <td>0.868500</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>0.861600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.494500</td>\n",
       "      <td>2.276714</td>\n",
       "      <td>11.038500</td>\n",
       "      <td>2.551000</td>\n",
       "      <td>10.303100</td>\n",
       "      <td>14.200400</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.458800</td>\n",
       "      <td>2.269240</td>\n",
       "      <td>10.910200</td>\n",
       "      <td>2.394200</td>\n",
       "      <td>10.169000</td>\n",
       "      <td>14.233100</td>\n",
       "      <td>0.867100</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>0.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.464900</td>\n",
       "      <td>2.263159</td>\n",
       "      <td>10.913900</td>\n",
       "      <td>2.342100</td>\n",
       "      <td>10.215400</td>\n",
       "      <td>14.337400</td>\n",
       "      <td>0.865400</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.476300</td>\n",
       "      <td>2.255595</td>\n",
       "      <td>11.775500</td>\n",
       "      <td>2.646300</td>\n",
       "      <td>11.098300</td>\n",
       "      <td>14.206500</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.856300</td>\n",
       "      <td>0.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.449700</td>\n",
       "      <td>2.249283</td>\n",
       "      <td>11.107500</td>\n",
       "      <td>2.372000</td>\n",
       "      <td>10.402700</td>\n",
       "      <td>14.077700</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>0.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.456700</td>\n",
       "      <td>2.246685</td>\n",
       "      <td>11.177900</td>\n",
       "      <td>2.280200</td>\n",
       "      <td>10.456400</td>\n",
       "      <td>14.664600</td>\n",
       "      <td>0.866600</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.442100</td>\n",
       "      <td>2.239460</td>\n",
       "      <td>11.006900</td>\n",
       "      <td>2.344800</td>\n",
       "      <td>10.347300</td>\n",
       "      <td>14.368100</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.426900</td>\n",
       "      <td>2.235990</td>\n",
       "      <td>11.089200</td>\n",
       "      <td>2.366200</td>\n",
       "      <td>10.470500</td>\n",
       "      <td>14.932500</td>\n",
       "      <td>0.865100</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.440600</td>\n",
       "      <td>2.231828</td>\n",
       "      <td>11.485700</td>\n",
       "      <td>2.419600</td>\n",
       "      <td>10.785900</td>\n",
       "      <td>14.378300</td>\n",
       "      <td>0.866900</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.412100</td>\n",
       "      <td>2.228310</td>\n",
       "      <td>11.333100</td>\n",
       "      <td>2.375700</td>\n",
       "      <td>10.660100</td>\n",
       "      <td>14.366100</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.407600</td>\n",
       "      <td>2.224597</td>\n",
       "      <td>11.712700</td>\n",
       "      <td>2.437000</td>\n",
       "      <td>10.980000</td>\n",
       "      <td>14.558300</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.413300</td>\n",
       "      <td>2.219937</td>\n",
       "      <td>11.360700</td>\n",
       "      <td>2.467000</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>13.987700</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.855700</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.424100</td>\n",
       "      <td>2.215797</td>\n",
       "      <td>11.463300</td>\n",
       "      <td>2.571900</td>\n",
       "      <td>10.722900</td>\n",
       "      <td>14.075700</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.411600</td>\n",
       "      <td>2.213666</td>\n",
       "      <td>11.614600</td>\n",
       "      <td>2.449100</td>\n",
       "      <td>10.882200</td>\n",
       "      <td>14.077700</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>0.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.418800</td>\n",
       "      <td>2.213008</td>\n",
       "      <td>11.261800</td>\n",
       "      <td>2.380500</td>\n",
       "      <td>10.594400</td>\n",
       "      <td>14.288300</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.855700</td>\n",
       "      <td>0.860400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.407400</td>\n",
       "      <td>2.210527</td>\n",
       "      <td>11.365300</td>\n",
       "      <td>2.411700</td>\n",
       "      <td>10.600800</td>\n",
       "      <td>14.147200</td>\n",
       "      <td>0.866800</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.406300</td>\n",
       "      <td>2.209181</td>\n",
       "      <td>11.664800</td>\n",
       "      <td>2.729400</td>\n",
       "      <td>10.942800</td>\n",
       "      <td>14.339500</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.394100</td>\n",
       "      <td>2.205841</td>\n",
       "      <td>11.788100</td>\n",
       "      <td>2.758400</td>\n",
       "      <td>11.060600</td>\n",
       "      <td>14.208600</td>\n",
       "      <td>0.866100</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.394300</td>\n",
       "      <td>2.205088</td>\n",
       "      <td>11.649100</td>\n",
       "      <td>2.742400</td>\n",
       "      <td>10.883900</td>\n",
       "      <td>14.259700</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.394800</td>\n",
       "      <td>2.203823</td>\n",
       "      <td>11.668300</td>\n",
       "      <td>2.773600</td>\n",
       "      <td>10.859600</td>\n",
       "      <td>14.431500</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.371600</td>\n",
       "      <td>2.202407</td>\n",
       "      <td>11.893100</td>\n",
       "      <td>2.754600</td>\n",
       "      <td>11.086700</td>\n",
       "      <td>14.308800</td>\n",
       "      <td>0.866600</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.401700</td>\n",
       "      <td>2.201693</td>\n",
       "      <td>11.973000</td>\n",
       "      <td>2.766400</td>\n",
       "      <td>11.173800</td>\n",
       "      <td>14.249500</td>\n",
       "      <td>0.867100</td>\n",
       "      <td>0.856700</td>\n",
       "      <td>0.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.421500</td>\n",
       "      <td>2.201226</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>2.805900</td>\n",
       "      <td>11.200500</td>\n",
       "      <td>14.229000</td>\n",
       "      <td>0.867300</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.861600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15120, training_loss=2.696330834444238, metrics={'train_runtime': 10620.0465, 'train_samples_per_second': 22.775, 'train_steps_per_second': 1.424, 'total_flos': 1.0869185553408e+17, 'train_loss': 2.696330834444238, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T13:03:30.348642Z",
     "iopub.status.busy": "2024-03-07T13:03:30.348163Z",
     "iopub.status.idle": "2024-03-07T13:05:35.771802Z",
     "shell.execute_reply": "2024-03-07T13:05:35.771178Z",
     "shell.execute_reply.started": "2024-03-07T13:03:30.348615Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdad0cb4a98f485aaac9dde3f28f9f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df534641fe243dfadf6a0f69b41d615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43786808606a4913b0861d4e8cd92630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Shakhovak/flan-t5-base-sheldon-chat-v2/commit/ae2a7ee107d9da041aa3be37333ec32b3e9d9f5a', commit_message='End of training', commit_description='', oid='ae2a7ee107d9da041aa3be37333ec32b3e9d9f5a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T13:05:35.772917Z",
     "iopub.status.busy": "2024-03-07T13:05:35.772555Z",
     "iopub.status.idle": "2024-03-07T13:31:26.189775Z",
     "shell.execute_reply": "2024-03-07T13:31:26.189018Z",
     "shell.execute_reply.started": "2024-03-07T13:05:35.772898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
